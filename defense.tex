%undeveloped code: RNN, sys, bo

\documentclass{beamer}
%\usepackage{cite} %\bibliographystyle{IEEEtran}
\usepackage[backend=bibtex,style=authortitle-ibid]{biblatex}
\bibliography{IEEEfull,library}
%\addbibresource{library.bib}
\usetheme{metropolis}           % Use metropolis theme
\title{Unsupervised Anomaly Detection in Sequences
Using Long Short Term Memory Recurrent Neural Networks}
\date{\today} %todo
\author{Majid S. alDosari}
\institute{George Mason University}


\input{custom}
\begin{document}

  \maketitle

  \begin{frame}{Contents}
    \tableofcontents 
  \end{frame}

  \section{Introduction}

  \begin{frame}{Modern technology facilitates the 
      capture, storage, and processing of sequential data at scale}

    \begin{itemize}
    \item Data capture
      \begin{itemize}
      \item physiological signals
      \item network traffic
      \item industrial processes
      \item automobiles
      \item website navigation
      \item environment
      \end{itemize}
    \item Data storage
      \begin{itemize}
      \item Hadoop
      \item MongoDB
      \end{itemize}
    \item Ubiquitous computing
      \begin{itemize}
      \item cloud/cluster
      \item desktop
      \item at point of capture
      \end{itemize}
    \end{itemize}    

  \end{frame}


  \begin{frame}{Problem: Finding anomalous data is challenging}

    \begin{itemize}
    \item large 
    \item varied
    \item domain knowledge required
    \end{itemize}

  \end{frame}


  \begin{frame}{Solution: Use recurrent neural networks to generically find anomalous data}

    \begin{itemize}
      \item This work:
    \begin{enumerate}
      \item Background: Anomaly detection in sequences
      \item Sequence Modeler: Recurent neural network (RNN)
      \item Experiments: RNNs for anomaly detection
    \end{enumerate}

    \item Prior: Malhotra \footcite{Malhotra2015} but no emphasis on process
    % i actually had a discussion with him
    \end{itemize}

  \end{frame}


  \section{The Challenge of Anomaly Detection in Sequences}

  \begin{frame}{Anomaly dectection work is fragmented}
    \begin{itemize}
    \item variety of solutions in communication networks, biology, economics, biology, ...etc.
    \item different settings
    \item no comparison between application domains
    \item technical basis in computer science vs. statistics
    \item not much review literature:
      Cheboli\footcite{Cheboli2010}
      and Gupta\footcite{Gupta2013}
    \end{itemize}
  \end{frame}


  \begin{frame}{Define the problem to focus on the right solution}

    Sequence $\x$
    \begin{gather*}
      \x=\{\x\tm{1},\x\tm{2},\x\tm{t},\ldots,\x\tm{T}\} \\
      \x\tm{t} \in \mathbb{R}^v
    \end{gather*}%

    Assumption: Anomalies are a small part of the data.

  \end{frame}


  \begin{frame}{Solution must answer the following:}

    \begin{enumerate}
    \item
      \textcolor{red}{What is normal (as an anomaly is defined as what is \textit{not} normal)?}
    \item
      What measure is used to indicate how anomalous point(s) are?
    \item
      How is the measure tested to decide if it is anomalous?
    \end{enumerate}

  \end{frame}


  \begin{frame}[allowframebreaks]{Solution must address different anomaly types}

    Simple point anomaly
    \centering
    \includegraphics[width=\textwidth]{figs/trivial.pdf}

    \framebreak
    Anomaly in a periodic context
    \centering
    \includegraphics[width=\textwidth]{figs/context.pdf}

    \framebreak
    Discord anomaly in a periodic time series
    \centering
    \includegraphics[width=\textwidth]{figs/discord_per.pdf}

    \framebreak
    Discord anomaly in an aperiodic time series
    \centering
    \includegraphics[width=\textwidth]{figs/discord_aper.pdf}

    \framebreak
    \centering
    Multivariate: (a)synchronous and (a)periodic

  \end{frame}


  \section{Procedure}

  \begin{frame}{Description of anomaly detection procedure is straightforward}

    \begin{enumerate}
    \item Compute an anomaly score for an observation
    \item Aggregate the anomaly scores for many observations.
    \item Use the anomaly scores to determine whether an observation can be considered anomalous
    \end{enumerate}

  \end{frame}

  \begin{frame}{Characterizing normal behavior is involved}

    \begin{enumerate}
    \item Extract Samples
    \item Transform Samples
    \item Apply Detection Technique
    \end{enumerate}

  \end{frame}


  \section{1. Sample Extraction}
  
  \begin{frame}{Use sliding windows to obtain samples}
    
    \begin{center}
    $\mathcal{X}=\{W_1,W_2,\ldots,W_p\}$
    \end{center}

    \begin{itemize}
      \item hop, $h$
      \item window, $w$ %for localizing anom
    \end{itemize}

  \end{frame}


  \begin{frame}{Problem: Hops can skip over anomalies}

    \begin{center} 
     sequence: \emph{abc\underline{c}abcabc} 

      \begin{tabular}{|c|c|}
        \hline
        hop ($h$) & Ordered Windows \\
        \hline
        \hline
        1 & \emph{abc},
            \emph{bc\underline{c}}, 
            \emph{c\underline{c}a}, 
            \emph{cab},
            \emph{abc}, 
            \emph{bca},
            \emph{cab},
            \emph{abc} \\
        \hline
        2 & \emph{abc},
            \emph{c\underline{c}a},
            \emph{abc},
            \emph{cab} \\
        \hline
        3 & \emph{abc}, 
            \emph{cab},
            \emph{cab} \\
        \hline
        4 & \emph{abc}, 
            \emph{abc} \\
        \hline
      \end{tabular}

    \end{center}
    
  \end{frame}


  \begin{frame}{Problem: Window size must be large enough to contain anomaly}

    \begin{center}
      sequence:
      \emph{aaabbbccc\underline{c}aaabbbcccaaabbbccc}
    \end{center}

    Window width must be at least 4.

  \end{frame}


  \begin{frame}{Problem: Treating window width as a dimension ignores temporal nature}

    \begin{gather*}
      \mathcal{X} =\{W_1,W_2,\ldots,W_p\}   \\
      W \in \mathcal{R}^{1 \times w}
    \end{gather*}
    
  \end{frame}


  % \begin{frame}{Problem: Windows and hop size can't be too lage}

  %   \begin{center}
  %     sequence:
  %     \emph{aaabbbccc\underline{c}aaabbbcccaaabbbccc}
  %   \end{center}

  % \end{frame}


  \section{2. Transformation}


  \begin{frame}{Transformation can help reveal anomalies}

    \begin{itemize}
    \item Haar transform %but only if 
    \item Symbolic Aggregate approXimation (SAX) \footcite{Lin2007}
    \end{itemize}

  \end{frame}

  \begin{frame}{Transformation is not general}

    \begin{itemize}
    \item Choice of representation must be compatible with data characteristics  %can't encode pt anom in freq
    \item normal:anomaly as transform(normal):transform(anomaly)
    \end{itemize}

    Study \footcite{Wang2013} suggests generally little difference among representations.

  \end{frame}


  \section{3. Detection Technique}

  \begin{frame}{Anomaly detection techniques and their application domains are varied}

    Based on
    \begin{itemize}
    \item Segmentation:
    \item Information Theory
    \item Proximity 
    \item Modeling
    \end{itemize}

  \end{frame}

  \begin{frame}{Model and proximity-based techniques are most developed}

    Based on
    \begin{itemize}
    \item Segmentation: requires homogeneous segments
    \item Information Theory: requires finding sensitive information-theoretic measure
    \item Proximity
    \item Modeling
    \end{itemize}

  \end{frame}


  \section{Proximity}


  \begin{frame}{Idealization never occurs}

      \centering
      \includegraphics[height=\textheight]{figs/simple_dist.pdf}

  \end{frame}

  \begin{frame}{Practically, distributions are complicated}
    
    \centering
    \begin{tabular}{c|c}
    \includegraphics[width=.45\textwidth]{figs/hard1_dist.pdf} &
    \includegraphics[width=.45\textwidth]{figs/hard2_dist.pdf}
    \end{tabular}

  \end{frame}


  \section{Effects on Point Distribution}


  \begin{frame}{Distance measure should be invariant to:}

    \begin{itemize}
      \item length
      \item (skew)
      \item translation
      \item (amplitude)
    \end{itemize}

    Study \footcite{Wang2013}: Not much difference in similarity measures

  \end{frame}

  \begin{frame}{Window width needs to be chosen on the scale of expected anomaly}
    
    % so you might thing oh just make it big
    If width is too large:
    \begin{itemize}
      \item anomalous points not distinguished
      \item data becomes equidistant in high-dimensional space
    \end{itemize}

  \end{frame}


  \section{Data Classification}

  \begin{frame}{Global vs Local: Local techniques use neigborhood data}
    
    \centering
    \begin{tabular}{c|c}
      \includegraphics[width=.45\textwidth]{figs/hard1_dist.pdf} &
      \includegraphics[width=.45\textwidth]{figs/hard2_dist.pdf}
    \end{tabular}

  \end{frame}


  \section{Nearest Neighbor}


  \begin{frame}{Overlapping windows distort data similarity}

    \begin{tabular}{lc}
      \emph{abcabcXXXabcababc} &
      \begin{small} 
 %       \begin{center}
          \begin{tabular}{|c||c|}
            \hline
            $h=1$ & $h=3$ \\
            \hline
            \hline
            \emph{abc} & \emph{abc} \\
            \emph{bca} & \\
            \emph{cab} & \\
            \hline
            \emph{abc} & \emph{abc} \\
            \emph{bcX} & \\
            \emph{cXX} & \\
            \hline
            \emph{XXX} & \emph{XXX} \\
            \emph{XXa} & \\
            \emph{Xab} & \\
            \hline
            \emph{abc} & \emph{abc} \\
            \emph{bca} & \\
            \emph{cab} & \\
            \hline
            \emph{aba} & \emph{aba} \\ 
            \emph{bab} & \\
            \emph{abc} & \\
            \hline
          \end{tabular}
%        \end{center}
      \end{small}
    \end{tabular}

  \end{frame}


  \begin{frame}{Solution: Use non-self matches \footcite{Keogh2005}}

    \small
    \centering
    \begin{tabular}{|c||c|}
      \hline
      $h=1$ & $h=3$ \\
      \hline
      \hline
      \emph{abc} & \emph{abc} \\
      \emph{bca} & \\
      \emph{cab} & \\
      \hline
      \emph{abc} & \emph{abc} \\
      \emph{bcX} & \\
      \emph{cXX} & \\
      \hline
      \emph{XXX} & \emph{XXX} \\
      \emph{XXa} & \\
      \emph{Xab} & \\
      \hline
      \emph{abc} & \emph{abc} \\
      \emph{bca} & \\
      \emph{cab} & \\
      \hline
      \emph{aba} & \emph{aba} \\ 
      \emph{bab} & \\
      \emph{abc} & \\
      \hline
    \end{tabular}

  \end{frame}


  \begin{frame}{kNN uses no local information}
    % hotsax k=1, go thru 2,3,*4*,5,6 
    % N2 different density
    
    \centering
    \begin{tabular}{c|c}
      \includegraphics[width=.45\textwidth]{figs/hard1_dist.pdf} &
      \includegraphics[width=.45\textwidth]{figs/hard2_dist.pdf}
    \end{tabular}

  \end{frame}


  \begin{frame}{Local Outlier Factor\footcite{Breunig1999} uses local density information}
    
    A point is likely to be an anomaly if its neighbors are in dense regions while it is in a less dense region
    \centering
    \begin{tabular}{c|c}
    \includegraphics[width=.45\textwidth]{figs/hard1_dist.pdf} &
    \includegraphics[width=.45\textwidth]{figs/hard2_dist.pdf}
    \end{tabular}
    (Didn't you say anomalies may not be in less dense regions?!)
    
  \end{frame}


  \section{Clustering}

  \begin{frame}{Clustering algorithms are usually not designed to find anomalies}

    Assumptions: anomalous points
    \begin{itemize}
    \item do not belong to a cluster (DBSCAN\footcite{Ester1996})
    \item are far from a cluster centroid %see the problem?
    \item are in less dense clusters
    \end{itemize}

  \end{frame}


  \section{Models}

  \begin{frame}{Hidden Markov Models (HMMs) are general advanced sequence modelers}
    
    Restrictions:
    \begin{itemize}
      \item fixed length sequences
      \item Markovian process
    \end{itemize}

  \end{frame}


  \section{Problems with Established Techniques}


  \begin{frame}{How to determine \emph{a priori} what the best algorithm is?}
    
    review papers only give subjective assessments

  \end{frame}


  \begin{frame}{Proximity-based techniques need alot of decisions}
    
    Choose:
    \begin{itemize}
      \item similarity measure
      \item sliding window size
      \item sliding window hop
      \item compatible classification technique
    \end{itemize}

  \end{frame}


  \begin{frame}{Solution: Use a model-based technique}
    
    \begin{itemize}
      \item characterize normal %more elegant. compare with clustering/NN
      \item restriction: use when data \emph{can} be modeled
    \end{itemize}

    Ideally:
    \begin{itemize}
      \item model arbitrary time series
      \item minimize effect of window length
      \item requires as few parameters as possible
    \end{itemize}

  \end{frame}





  \section{Recurrent Neural Networks (RNNs)}

  
  \begin{frame}{RNNs are powerful}
    
    \begin{itemize}

    \item speech recognition
    \item handwriting recognition
    \item music generation
    \item text generation
    \item handwriting generation
      %lstm
    \item translation
    \item identifying non-verbal cues from speech
    \item image caption generation
    \item video to text description
    \item generating talking heads

    \end{itemize}
        
  \end{frame}


  \begin{frame}{RNNs are more flexible and efficient than HMMs}
    
    state:
    \begin{itemize}
    \item HMM: hidden state depends only on previous state
    \item RNN: shared state
    \end{itemize}

    generality:
    \begin{itemize}
    \item HMM: Markovian
    \item RNN: general computation device
    \end{itemize}
    
  \end{frame}


  \begin{frame}{Recurrence explains the efficiency of RNN encoding}
    
    \newsavebox{\recurrent}
\savebox{\recurrent}{\includegraphics[]{figs/recurrent.pdf}}
\newsavebox{\recurrentuf}
\savebox{\recurrentuf}{\includegraphics[]{figs/recurrent_uf.pdf}}

cyclic view
    \begin{center} \usebox{\recurrent} \end{center}

acyclic view
    \begin{center}  \usebox{\recurrentuf} \end{center}
    
  \end{frame}


  \begin{frame}{RNN computation is elaborate}

    \newsavebox{\rnn}
    \savebox{\rnn}{\includegraphics[]{figs/rnn.pdf}}
    \newsavebox{\rnnuf}
    \savebox{\rnnuf}{\includegraphics[]{figs/rnn_uf.pdf}}

    \begin{tabular}{p{\wd\rnn}|p{\wd\rnnuf}}
      \includegraphics[width=.7\wd\rnn]{figs/rnn.pdf}  & 
                                                         \includegraphics[width=.7\wd\rnnuf]{figs/rnn_uf.pdf}  
    \end{tabular}

  \end{frame}


  \begin{frame}{Training RNNs is difficult}
    
    \begin{equation*}
      L(\vc{o},\vc{y}) = \frac{1}{TV} \sum_t \sum_v(
      \vc{o}\tm{t}_v - \vc{y}\tm{t}_v
      )^2
    \end{equation*}

    \begin{itemize}
      \item but mini-batch SGD-flavor training still works
        \begin{equation*}
          \Delta \vc{\theta} =
          -\alpha
          \frac{1}{|M|} \sum_{(\vc{x}_m,\vc{y}_m) \in M}
          \frac{\partial{L}(
            \vc{o}%(\x_m;\vc{\theta})
            ,\vc{y}_m)}
          {\partial{\vc{\theta}}}
        \end{equation*}
      \item acute vanishing gradient problem
        \begin{equation*}
          \frac{\partial{L}\tm{t}}{\partial{\vc{W}_{ss}}} = 
          \sum_{i=0}^T
          \frac{\partial{    L}\tm{ t}}{\partial{\vc{o}\tm{t}}}
          \frac{\partial{\vc{o}}\tm{t}}{\partial{\vc{s}\tm{t}}}
          \left(
            \prod_{j=i+1}^{T}
            \frac{\partial{\vc{s}}\tm{j}}{\partial{\vc{s}\tm{j-1}}}
          \right)
          \frac{\partial{\vc{s}}\tm{i}}{\partial{\vc{W}_{ss}}}
        \end{equation*}
      \end{itemize}

    \end{frame}


    \begin{frame}{Understand vanishing gradient problem through computational graph for $T=4$}

      \includegraphics[width=\textwidth]{figs/grad.pdf}

    \end{frame}


    \begin{frame}{Long Short Term Memory (LSTM) `cells' store information but are more complicated than vanilla RNNs' $tanh$}

      \includegraphics[width=\textwidth]{figs/lstm.pdf}

    \end{frame}

% endp

%  \begin{frame}[allowframebreaks]{References}
%    %
%  \end{frame}

% /other/ contribs

\end{document}

