
\chapter[]{Recurrent Neural Networks}

\section{Introduction}

In the previous chapter, an argument was made for the use of models to for the purpose of anomaly detection. In this chapter, recurrent neural networks \cite{Rumelhart1986}, or RNNs, are introduced as powerful sequence modelers of arbitrary length.

RNNs have acheived state-of-the-art performance in supervised tasks such as handwriting recognition \cite{Graves2009}, and speech recognition  \cite{Graves2013}. RNNs can also be trained unsupervised to generate  music  \cite{Boulanger-Lewandowski2012}, text \cite{Martens2011a,Graves2013b}, and handwriting \cite{Graves2013b}. Given these impressive feats, RNNs can be effectively used for anomaly detection in sequences \cite{Marchi2015,Malhotra2015}.

Like HMMs, RNNs have hidden states. But, in contrast to HMMs, RNNs make a more efficient use of its hidden state \cite{ZacharyC.Lipton2015}. In a HMM, the hidden state depends only on the previous state which must store possible configurations of the sequence. So to incorporate information about a window of previous states, the hidden state must grow exponentially large making them computationally impractical. While in RNNs, the state is shared over time; the hidden state of a RNN contains information from an arbitrarily long window. The next section will explain this.

In addition, RNNs do not make a Markovian assumption. In fact, they are so general, that RNNs have been shown to be equivalent to finite state automata \cite{Minsky1967}, equivalent to turing machines \cite{Siegelmann1995}, and more powerful than turing machines \cite{Siegelmann1993} even.

In the next few sections, some essential concepts of RNNs will be presented as most applicable to this work. Consult the RNN chapter in \cite{Bengio-et-al-2015-Book} for an in-depth treatment.

The concepts presented are accompanied by mathematical expressions although only concepts will be presented. Due to the preciceness needed to follow the expressions, some conventions are followed: Due to the numerous interacting quantities, a typographic distinction is made between vectors ($\vc{v}$) and  matrices ($\mt{M}$). Variables are time indexed with superscripts enclosed in parentheses ($\vc{x}^{(t)}$) leaving subscripts available to index other quantities.

% notation:
% matrix: bold upright 
% vector: italic bold
% funcs: 'normal' asis in mathmode.

\section{Recurrence}

Recurrence explains how a RNN stores a distributed state. Consider a dynamical system driven by signal $\vc{x}^{(t)}$ as in Equation \ref{eqn:ds}.

\begin{equation}
  \vc{s}^{(t)} = f(\vc{s}^{(t-1)},\vc{x}^{(t)};\vc{\theta})
  \label{eqn:ds}
\end{equation}

The state, $\vc{s}\tm{t}$, depends on the previous state, $\vc{s}^{(t-1)}$, through some function $f$ parameterized by $\vc{\theta}$. There is no restriction on the number of previous time steps. For example, for four previous time steps
\begin{equation*}
\vc{s}\tm{t} = 
f(f(f(f(\s\tm{t-4}
,\x\tm{t-3};\vc{\theta})
,\x\tm{t-2};\vc{\theta})
,\x\tm{t-1};\vc{\theta})
,\x\tm{t};\vc{\theta})     .
\end{equation*}

So the composite function, $g$, can be written as depending on an arbitrary number, $T$, of time steps.
\begin{equation*}
\s\tm{T} = g_{T}(\x\tm{T},\x\tm{T-1},\x\tm{T-2},\ldots,\x\tm{2},\x\tm{1})
\end{equation*}
In other words, the vector $\s\tm{T}$ contains a summary of the of the preceding sequence, $(\x\tm{T},\x\tm{T-1},\x\tm{T-2},\ldots,\x\tm{2},\x\tm{1})$, through $g_T$.

It can be difficult to follow recurrent computations by looking at mathematical expressions. So recurrence can be graphically represented in two ways. One way, shown in Figure \ref{fig:recurrent}, shows the state feeding back into itself through its parameters representing Equation \ref{eqn:ds}. The other way is to `unfold' the recurrence in a flow graph as in Figure \ref{fig:recurrent_uf}. Graphs offer a convenient way of organizing computations. The (unfolded) graph shows every hidden state, say $\s\tm{t}$, is dependent on the current input, $\x\tm{t}$, the previous state, $\s\tm{t-1}$, and (fixed) parameters $\vc{\theta}$. So it should be obvious that $\vc{\theta}$ is shared over successive computations of $\s$.

\begin{figure}[H]
  \centering
  \begin{subfigure}[]{.2\textwidth}
    \begin{center}
    \includegraphics[]{figs/recurrent.pdf}
    \end{center}
    \caption{cyclic}
    \label{fig:recurrent}
  \end{subfigure}
  \begin{subfigure}[]{.79\textwidth}
    \begin{center}
    \includegraphics[]{figs/recurrent_uf.pdf}
    \end{center}
    \caption{acyclic (unfolded)}
    \label{fig:recurrent_uf}
  \end{subfigure}
  \caption{Recurrence Graph Views}
  \label{fig:recurrence}
\end{figure}


\section{Basic Recurrent Neural Network}

The functionality of a basic RNN resembles that of a traditional neural network with the addition of a state variable shared over time.


The core RNN functionality can be formulated as Equations \ref{eqn:rnn} describe. Each equation represents a `layer' or `node' in the network.
% no new para
\begin{subequations}  
  \label{eqn:rnn}
  \begin{align}
    \s\tm{t}&=
              \mathrm{\tanh}(
              \mt{W}_{ss}
              \s\tm{t-1}
              +
              \mt{W}_{xs}
              \x\tm{t}
              +
              \vc{b}_s
              ) 
              \label{eqn:rnns} \\ 
    \vc{o}\tm{t}&=
             \mt{W}_{so}
             \s\tm{t}
             +
             \vc{b}_{o} \label{eqn:rnno}
  \end{align}
\end{subequations}
\noindent 
The output at time $t$, $\vc{o}\tm{t}$, depends on current state, $\s\tm{t}$, which, in turn, depends on the previous state, $\s\tm{t-1}$, and the current input, $\x\tm{t}$, through associated weight matrices, $\mt{W}$. The weight matrices are subscripted with two variables to associate an input with an output. The input variable is the first subscript while the output variable is the second. For example, $\mt{W}_{so}$ connects the input from $\s$ to output, $\vc{o}$. Also, bias vectors, $\vc{b}$ allow values to be adjusted additively (offset). Finally, the hyperbolic tangent, $\tanh$, provides a non-linearity that allows the RNN to summarize arbitrarily complex sequences.

The output, $\vc{o}$, needs to be need to be compared to a `target', $\vc{y}$, through a loss function, $L$. For predicting sequences as targets, the mean squared error is a common choice for the loss function. Squared differences are summed and averaged over and the number of variables of the sequence resulting in a scalar as shown in Equation \ref{eqn:mse}.

\begin{equation}
  \label{eqn:mse}
  L(\vc{o},\vc{y}) = \frac{1}{TV} \sum_t \sum_v(
  \vc{o}\tm{t}_v - \vc{y}\tm{t}_v
  )^2,
\end{equation}
\noindent
$V$ and $T$ are the number of variables and the length of the sequences respectively indexed by $t$ and $v$ respectively.

Equations \ref{eqn:mse} and \ref{eqn:rnn}, together, define the framework for a basic RNN. One way to enhance this is to `stack' states so that the output from one state is input into another state in the same time step. So the last, $n$th, layer produces $\vc{o}$.  There is evidence that stacking the state layers leads to the network being able to learn time series at multiple time scales \cite{Hermans2013} \cite{Pascanu2013a}.

The stacking can be seen graphically in \ref{fig:rnn}.

\begin{figure}[H]
  \centering
  \begin{subfigure}[]{.2\textwidth}
    \begin{center}
    \includegraphics[]{figs/rnn.pdf}
    \end{center}
    \caption{cyclic}
    \label{fig:rnn_f}
  \end{subfigure}
  \begin{subfigure}[]{.79\textwidth}
    \begin{center}
    \includegraphics[]{figs/rnn_uf.pdf}
    \end{center}
    \caption{acyclic}
    \label{fig:rnn_uf}
  \end{subfigure}
  \caption{Recurrent Neural Network Graph Views}
  \label{fig:rnn}
\end{figure}
\noindent
Bias terms are omitted for clarity but are included as elements in $\vc{\theta}$ along with elements from $\W$. The notation of $\vc{\theta}$ follows that of $\mt{W}$ but captures more variables by vectorizing each component for concatenation into a single vector. $\vc{\theta}_{ss} = (\vc{W}_{ss},\vc{b}_s)$, $\vc{\theta}_{xs} = (\vc{W}_{xs})$, and $\vc{\theta}_{so} = (\vc{W}_{so},\vc{b}_o)$ (mathematical operations to produce the vector are implied).

Notice that the unfolded view allows one to see that information can flow through many paths in time and layers (horizontally and vertically respectively). So `depth' can be used to describe the number of operations in these two directions with depth in the time direction being typically much greater. This makes RNNs, among `deep learning' architectures, the deepest networks!


\section{Training}

%use s_(l)ayers. todo

To train the network, the loss function is used to  minimize (optimized) an objective function given pairs of training examples, $(\x,\vc{y})$, by adjusting the parameters $\vc{\theta}$. Unfortunately, training neural networks in general, let alone recurrent neural networks, are challenging optimization and computational problems \cite{Blum1992}. However, a flavor of the relatively simple mini-batch stochastic gradient descent (SGD) has remained successful in training a variety of networks \cite{Bengio2012b} despite the availability of other, perhaps more sophisticated, optimization algorithms. %practical recs for training deep. todo 2014+ ref?

In plain mini-batch SGD, parameters are updated with a `learning reate', $\alpha$, for a selected `mini-batch' example set (from the training set), $M$, according to Equation \ref{eqn:sgd} until a convergence critereon is met.

\begin{equation}
  \label{eqn:sgd}
   \Delta \vc{\theta} =
   -\alpha
   \frac{1}{|M|} \sum_{(\vc{x}_m,\vc{y}_m) \in M}
   \frac{\partial{L}(\vc{o}(\x_m;\vc{\theta}),\vc{y}_m)}
   {\partial{\vc{\theta}}}
\end{equation}
%todo: make  summation bigger

Unfortunately, RNNs can suffer acutely from the vanishing (and exploding) gradient problem which makes learning long range dependencies difficult \cite{Hochreiter,Bengio1994,Doya1992,Pascanu2013c}.

The problem is understood through the calculation of $\partial{L}/\partial{\vc{W}_{ss}}$ for a `history' of $T$ points prior to $t$.
An involved application of the (backpropagated) differential chain rule to the loss function using the \emph{basic} (unfolded) RNN defined in Equations \ref{eqn:rnn} leads to Equation \ref{eqn:grad}.

\begin{equation}
  \label{eqn:grad}
  \frac{\partial{L}\tm{t}}{\partial{\vc{W}_{ss}}} = 
  \sum_{i=0}^T
  \frac{\partial{    L}\tm{ t}}{\partial{\vc{o}\tm{t}}}
  \frac{\partial{\vc{o}}\tm{t}}{\partial{\vc{s}\tm{t}}}
  \left(
    \prod_{j=i+1}^{T}
    \frac{\partial{\vc{s}}\tm{j}}{\partial{\vc{s}\tm{j-1}}}
  \right)
  \frac{\partial{\vc{s}}\tm{i}}{\partial{\vc{W}_{ss}}}
\end{equation}
\noindent
Equation \ref{eqn:grad} shows that there are multiplicative `chains' of differentials for each preceding time step. 

The computational graph in Figure \ref{fig:grad}, represents Equation \ref{eqn:grad} for $T=4$.

\begin{figure}[H]
  \centering
  \includegraphics[]{figs/grad.pdf}
  \caption{Gradient Calculation for a Basic RNN}
  \label{fig:grad}
\end{figure}
\noindent
Several quantities are distinguished. Dash-dotted lines are the independent inputs, $\x$, $\vc{y}$, and $\vc{W}$, to the loss function, $L$, while the dashed lines represent intermediate quantities in the graph. 

then show ch10 ways to deal with them.
most notable. lstm. HF opt. 

1st order 2nd order 

'Equilibrated adaptive learning rates for non-convex
optimization' -rmsprop is good. sgd good but issue is just how to adjust weights.

although second order. 1st order still competitive.


lstm:
CECs are
the main reason why LSTM nets can learn to discover the importance of (and memorize) events that
happened thousands of discrete time steps ago, while previous RNNs already failed in case of minimal
time lags of 10 steps


for detailed trning: 'How large should the batch size be for stochastic gradient descent?'



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
