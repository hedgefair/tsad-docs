\chapter[]{Anomaly Detection Using Recurrent Neural Networks}


\section{Introduction}

Chapters \ref{ch:ad} and \ref{ch:rnn}, separately, introduced anomaly detection in time series and recurrent neural networks as time series modelers.
%
This chapter outlines a procedure for finding anomalies using RNNs with the goal of mitigating many of the problems associated with anomaly detection that were discussed in \ref{ch:ad}.
%
As much as possible, the same procedure is applied for each time series used to test the procedure.


The outlines describes:
%
\begin{description}
%
\item[sampling:] the time series used for training and its associated sampling process
%
\item[recurrent autoencoders:] the specific form of the RNN
%
\item[training:] the training algorithm used on the RNN
%
\item[Bayesian optimization:] the search for optimized parameters for the procedure
%
\end{description}



\section{Sampling}


Some univariate time series \emph{with} anomalies were chosen or generated to test the anomaly detection process.
%
While there is only one `master' time series to train on (in each test), the RNN needs to see many `samples' from the master time series.
%
The sliding window procedure, described in \ref{sec:adsample}, is used to get samples of some window length.
%
Additionally, since RNNs do not need a fixed window, sliding windows are obtained for more than one window length.
%
Furthermore, the samples are collected into `mini-batches' (of the same window length) to be more compatible with the training procedure.
%
The window length is incremented (size skip) from some minimum until it is not possible to obtain a mini-batch (so the largest window will be close to the length of the time series).


Table \ref{tbl:winspec} specifies the relevent sizes and increments for the sampling process.
%
The values were adjusted manually until a `reasonable' number of samples were found.
%
However, the minimum window length was chosen such that meaningful dynamics were captured (regardless of the scale of the anomaly).

\begin{table}[H]
\centering
\begin{tabular}{|l||c|c|c|c|c||c|}
  \hline
  series & length & min. win. & slide skip & size skip & batch size & $\Rightarrow$ num. samples
  \\ \hline \hline
  sine (gen.) & 1000 & 100 & 10 & 10 & 30 & 96 
  \\ \hline
  ecg \cite{PhysioNet} & 3277 & 300 & 20 & 20 & 30 & 300
  \\ \hline
  spike (gen.) & 800 & 80 & 8 & 8 & 10 & 378
  \\ \hline
  power \cite{Keogh2005} & 7008 & 100 & 100 & 20 & 30 & 252
  \\ \hline
  sleep \cite{this} & 2560 & 300 & 20 & 20 & 30 & 165
  \\ \hline
\end{tabular}
\caption[]{Time series sample specifications} %todo: what case?
\label{tbl:winspec}
\end{table}


\section{RNN Setup: Autoencoder}

The RNN needs to learn what normal time series behaviour is.
%
So an autoencoder is used which can learn expected behavior by setting the target, $\vc{y}$, to be the input, $\vc{x}$.
%
The loss function is MSE (Equation \ref{eqn:mse}).
%
Furthermore, to prevent the RNN from learning trivial identity functions, Gaussian noise is added to the input where the standard deviation is equal to .75 the standard deviation of the whole time series.

\begin{equation*}
 \tilde{\x} = \x
 + \mathcal{N}(0,(0.75\sigma_{\mathrm{std}}(\x))^2)
\end{equation*}
\noindent
%
Note the comparison in the loss function is between the (uncorrupted) signal, $\x$, and the output from the network, $\vc{o}$, given $\x$: $L(\vc{o}(\tilde{\x}),\x)$.
%
With this setup, a denoising autoencoder, the data generating process, $p(\x)$, is implictly learned \cite{Bengio2013}.


\section{Training}


SGD with RMSprop \cite{Tieleman2012} for weight updates has been demonstrated to provide results that are similar to more sophisticated second-order methods but with significantly less computational cost \cite{Dauphin}.
%
Another benefit of RMSprop is that it is designed to work with mini-batch learning.
%
Since the training data is highly redunant (from sliding windows), it is expected that computing the gradient (to update parameters) from a subset of the data (mini-batch) is more efficient than computing the gradient for all the data.


For each derivative in the gradient, RMSprop keeps an exponentially-weighted moving average of derivative magnitudes which normalizes the gradient by its root-mean-squared.

rms prop: div learning rate for a weight by a running avg of recent gradiants

alpha is learning rate
rms_halflife=14
rms_reg=1e-8

1. split ts into 75pct for trainig 25 validation
2. shuffle trning data
2a. for each data in trning epoch
for each param p in theta
calc grad/der
update for each params_i
keep lowest validation
stop if improvement in validation is less than .005 5 times in a row

spec details in ref gen seq w rnn

theanets.

for detailed overall trning: 'How large should the batch size be for stochastic gradient descent?'


'advances in optimizing recurrent nn' enhanced sgd still competitive or better
'Equilibrated adaptive learning rates for non-convex optimization' -rmsprop is good. sgd good but issue is just how to adjust weights.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
